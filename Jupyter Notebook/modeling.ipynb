{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "movies = pd.read_csv(\"../datasets/ml-25m/movies.csv\")\n",
    "ratings = pd.read_csv(\"../datasets/ml-25m/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy encode genre \n",
    "genres_encoded = movies['genres'].str.get_dummies('|')\n",
    "\n",
    "# merge with original\n",
    "df_encoded = pd.concat([movies, genres_encoded], axis=1)\n",
    "\n",
    "# drop genre and no genre listed columns\n",
    "df_encoded.drop(columns=[\"genres\",\"(no genres listed)\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column for year\n",
    "df_encoded['year'] = df_encoded['title'].str.extract(r'\\((\\d{4})\\)')\n",
    "df_encoded['year'] = df_encoded['year'].fillna(0).astype('int64')\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rating and encoded dataframes \n",
    "merged = ratings.merge(df_encoded,on=\"movieId\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop timestamp column \n",
    "merged.drop(columns=\"timestamp\",inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display NA values  \n",
    "merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sample of 100k records from merged dataset\n",
    "merged_sample = merged.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating is a float number, to pass it to model, we need integer value\n",
    "encoder = {\n",
    "          0.5: 1,\n",
    "          1 : 2,\n",
    "          1.5: 3,\n",
    "          2:4,\n",
    "          2.5 : 5,\n",
    "          3:6,\n",
    "          3.5:7,\n",
    "          4:8,\n",
    "          4.5:9,\n",
    "          5:10\n",
    "           }\n",
    "merged_sample[\"rating\"] = merged_sample[\"rating\"].map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select predictor columns\n",
    "columns_X = merged_sample.columns.to_list()\n",
    "columns_X.remove(\"rating\")\n",
    "columns_X.remove(\"title\")\n",
    "columns_X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare X and y values \n",
    "X = merged_sample[columns_X]\n",
    "y = merged_sample[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select few algorithms for comparison \n",
    "models = []\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('DT', DecisionTreeRegressor()))\n",
    "models.append((\"RF\", RandomForestClassifier()))\n",
    "models.append(('NB', GaussianNB()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , \n",
    "                                   random_state=104,  \n",
    "                                   test_size=0.3,  \n",
    "                                   shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale training ans testing set \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train_scaled = scaler.transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform stratified k fold cross validation to see, which model performs the best \n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  kfold = StratifiedKFold(n_splits=5, random_state=104, shuffle=True)\n",
    "  cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplot of results for each algorithm \n",
    "pyplot.boxplot(results, labels=names)\n",
    "pyplot.title('Algorithm Comparison')\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that LDA performed the best, we can now try to optimise it with grid search\n",
    "parameters = {\n",
    "    'solver': ['svd'],\n",
    "    'n_components': [None, 1, 2, 3],\n",
    "    'tol': [1e-4, 1e-3, 1e-2],\n",
    "    'store_covariance': [True, False],\n",
    "}\n",
    "\n",
    "# Initialize LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(lda, parameters, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create our model with parameters that achieved the best score \n",
    "model = LinearDiscriminantAnalysis(**grid_search.best_params_)\n",
    "y_pred_test = model.fit(X_train,y_train).predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have chosen and optimised one of models available in sklearn package.  \n",
    "In the next part, we will compare it to algorithm obtained from Suprise package, which was influenced by SVD algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
